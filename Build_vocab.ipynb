{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of Build_vocab function using Concurrency Methods(Threading, Multiprocessing) to improve tokenization of captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from threading import Lock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def get_id(self, w):\n",
    "        return self.word2idx[w]\n",
    "    \n",
    "    def encode_seq(self, l):\n",
    "        return [self.word2idx[i] if i in self.word2idx else self.word2idx['<unk>'] for i in l]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "    def decode_seq(self, l):\n",
    "        return [self.idx2word[i] for i in l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary using Mulitprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(ann_file = '../flickr30k/results_20130124.token', threshold = 4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    punc_set = set([',',';',':','.','?','!','(',')'])\n",
    "    counter = Counter()\n",
    "    caption_list = []\n",
    "    split = pickle.load(open('train_set.p', 'rb'))\n",
    "    ann_file = os.path.expanduser(ann_file)\n",
    "    with open(ann_file) as fh:\n",
    "        for line in fh:\n",
    "            img, caption = line.strip().split('\\t')\n",
    "            if img[:-2] in split:\n",
    "                caption_list.append(caption)\n",
    "    for caption in tqdm(caption_list):\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        tokens = [elem for elem in tokens if elem not in punc_set] \n",
    "        counter.update(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    vocab.add_word('<break>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141960/141960 [00:25<00:00, 5472.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Non-optimized version: 26.20974588394165secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "build_vocab()\n",
    "\n",
    "end_time = time()        \n",
    " \n",
    "print(\"Time for Non-optimized version: %ssecs\" % (end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiprocessing with N=4 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(ann_file = '../flickr30k/results_20130124.token', threshold = 4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    punc_set = set([',',';',':','.','?','!','(',')'])\n",
    "    counter = Counter()\n",
    "    caption_list = []\n",
    "    split = pickle.load(open('train_set.p', 'rb'))\n",
    "    ann_file = os.path.expanduser(ann_file)\n",
    "    with open(ann_file) as fh:\n",
    "        for line in fh:\n",
    "            img, caption = line.strip().split('\\t')\n",
    "            if img[:-2] in split:\n",
    "                caption_list.append(caption)\n",
    "                \n",
    "    pool = mp.Pool(4)\n",
    "    tokens = pool.map(nltk.tokenize.word_tokenize, [caption.lower() for caption in tqdm(caption_list)])\n",
    "    pool.close()\n",
    "    tokens = [item for elem in tokens for item in elem]\n",
    "    tokens = [elem for elem in tokens if elem not in punc_set]\n",
    "    counter = Counter(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    vocab.add_word('<break>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141960/141960 [00:00<00:00, 875697.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Multiprocessing with N=4 processes: 12.161671161651611secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "build_vocab()\n",
    "\n",
    "end_time = time()        \n",
    " \n",
    "print(\"Time for Multiprocessing with N=4 processes: %ssecs\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threading with N=4 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(ann_file = '../flickr30k/results_20130124.token', threshold = 4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    punc_set = set([',',';',':','.','?','!','(',')'])\n",
    "    counter = Counter()\n",
    "    caption_list = []\n",
    "    split = pickle.load(open('train_set.p', 'rb'))\n",
    "    ann_file = os.path.expanduser(ann_file)\n",
    "    with open(ann_file) as fh:\n",
    "        for line in fh:\n",
    "            img, caption = line.strip().split('\\t')\n",
    "            if img[:-2] in split:\n",
    "                caption_list.append(caption)\n",
    "                \n",
    "    \n",
    "    global tokens \n",
    "    tokens = []\n",
    "    lock = Lock()\n",
    "\n",
    "    class DownloadWorker(Thread):\n",
    "        def __init__(self, queue):\n",
    "            Thread.__init__(self)\n",
    "            self.queue = queue\n",
    "        def run(self):\n",
    "            while True:\n",
    "                caption = self.queue.get()\n",
    "                lock.acquire()\n",
    "                tokens.append(nltk.tokenize.word_tokenize(caption))\n",
    "                lock.release()\n",
    "                self.queue.task_done()      \n",
    "    queue = Queue()\n",
    "    for _ in range(4):\n",
    "        worker = DownloadWorker(queue)\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "\n",
    "    for caption in caption_list:\n",
    "        queue.put(caption.lower())\n",
    "    \n",
    "    queue.join()\n",
    "    \n",
    "    tokens = [item for elem in tokens for item in elem]\n",
    "    tokens = [elem for elem in tokens if elem not in punc_set]\n",
    "    counter = Counter(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    vocab.add_word('<break>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Threading with N=4 threads: 36.773322105407715secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "build_vocab()\n",
    "\n",
    "end_time = time()        \n",
    " \n",
    "print(\"Time for Threading with N=4 threads: %ssecs\" % (end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the timings above, the most efficient method is multiprocessing with N=4 processes. It took 12.16 seconds to generate the counter word pairs (while dropping the uncommon words) as compared to non optimized version which took 26.2 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
